{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from typing import Any\n",
    "\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "import flax.struct\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import ml_collections\n",
    "import optax\n",
    "from utils.encoders import GCEncoder, encoder_modules\n",
    "from utils.flax_utils import ModuleDict, TrainState, nonpytree_field\n",
    "from utils.networks import MLP, GCActor, GCDiscreteActor, GCValue, Identity, LengthNormalize\n",
    "from utils.q_mixer_networks import QFunctionMixerCore\n",
    "\n",
    "\n",
    "# The QMixer agent is defined as a PyTreeNode so it can easily be updated/replaced.\n",
    "class QMixerAgent(flax.struct.PyTreeNode):\n",
    "    \"\"\"QMixer agent\"\"\"\n",
    "    rng: Any\n",
    "    network: Any\n",
    "    config: Any = nonpytree_field()\n",
    "\n",
    "    @jax.jit\n",
    "    def total_loss(self, batch, grad_params, rng=None):\n",
    "        \"\"\"Compute the total loss.\"\"\"\n",
    "        info = {}\n",
    "        observations = batch['observations']  # shape: (B, V, D)\n",
    "        next_observations = batch['next_observations']\n",
    "        actions = batch['actions']    # ground truth or teacher-forcing tokens\n",
    "        goals = batch['value_goals']\n",
    "        \n",
    "        if len(observations.shape) == 2:\n",
    "            observations = jnp.expand_dims(observations, axis=1)\n",
    "            next_observations = jnp.expand_dims(next_observations, axis=1)\n",
    "            actions = jnp.expand_dims(actions, axis=1)\n",
    "            goals = jnp.expand_dims(goals, axis=1)\n",
    "\n",
    "        actions = continuous_to_discrete(actions, self.config['action_max'], self.config['action_min'], self.config['num_bins'])\n",
    "\n",
    "        rewards = batch['rewards']\n",
    "        rewards = jnp.expand_dims(rewards, axis=-1)\n",
    "\n",
    "        current_dist, _ = self.network.select('q_predictors')(observations, goals, action_seq=actions, params=grad_params)\n",
    "\n",
    "        current_q, _ = self.network.select('target_q_predictors')(observations, goals, action_seq=None)\n",
    "        next_q, _ = self.network.select('target_q_predictors')(next_observations, goals, action_seq=None)\n",
    "\n",
    "        current_q_max = current_q.max(axis=-1)\n",
    "        next_q_max = next_q.max(axis=-1)\n",
    "\n",
    "        td_targets = jnp.zeros_like(next_q_max)\n",
    "\n",
    "        # All dimensions except last use next action dim\n",
    "        # print(td_targets[..., -1].shape, rewards.shape, next_q_max[..., 0].shape)\n",
    "        td_targets = td_targets.at[..., :-1].set(current_q_max[..., 1:])\n",
    "        td_targets = td_targets.at[..., -1].set(rewards + self.config['discount'] * next_q_max[..., 0])\n",
    "\n",
    "        #TODO: Implement mc_returns\n",
    "        if not self.config['gc_negative']:\n",
    "            mc_returns = batch['mc_returns']\n",
    "            td_targets = jnp.maximum(td_targets, mc_returns)\n",
    "\n",
    "        action_mask = jax.nn.one_hot(actions, num_classes=self.config['num_bins']).astype(jnp.float32)\n",
    "\n",
    "        current_q = (current_dist * action_mask).sum(axis=-1)\n",
    "        td_error = 0.5 * jnp.mean((current_q - td_targets) ** 2)\n",
    "\n",
    "        # Conservative regularization\n",
    "        non_action_mask = 1 - action_mask\n",
    "        non_action_q = (current_dist ** 2 * non_action_mask).sum(axis=-1)\n",
    "        conservative_loss = (self.config['alpha'] / (2 * (current_dist.shape[-1] - 1))) * non_action_q.mean()\n",
    "\n",
    "        info[\"td_error\"] = td_error\n",
    "        info[\"conservative_loss\"] = conservative_loss\n",
    "        info[\"total_loss\"] = td_error + conservative_loss\n",
    "\n",
    "        loss = td_error + conservative_loss\n",
    "        \n",
    "        return loss, info\n",
    "\n",
    "    def target_update(self, network, module_name):\n",
    "        \"\"\"Update the target network.\"\"\"\n",
    "        new_target_params = jax.tree_util.tree_map(\n",
    "            lambda p, tp: p * self.config['tau'] + tp * (1 - self.config['tau']),\n",
    "            self.network.params[f'modules_{module_name}'],\n",
    "            self.network.params[f'modules_target_{module_name}'],\n",
    "        )\n",
    "        network.params[f'modules_target_{module_name}'] = new_target_params\n",
    "\n",
    "    @jax.jit\n",
    "    def update(self, batch):\n",
    "        \"\"\"Update the agent and return a new agent with accompanying info.\"\"\"\n",
    "        new_rng, rng = jax.random.split(self.rng)\n",
    "\n",
    "        def loss_fn(grad_params):\n",
    "            return self.total_loss(batch, grad_params, rng=rng)\n",
    "        \n",
    "        new_network, info = self.network.apply_loss_fn(loss_fn=loss_fn)\n",
    "        self.target_update(new_network, 'q_predictors')\n",
    "\n",
    "        return self.replace(network=new_network, rng=new_rng), info\n",
    "\n",
    "    @jax.jit\n",
    "    def sample_actions(\n",
    "        self,\n",
    "        observations,\n",
    "        goals=None,\n",
    "        seed=None,\n",
    "        temperature=1.0,\n",
    "    ):\n",
    "        \"\"\"Sample actions from the actor network.\"\"\"\n",
    "        if len(observations.shape) == 1:\n",
    "            observations = jnp.expand_dims(observations, axis=0)\n",
    "            observations = jnp.expand_dims(observations, axis=1)\n",
    "            goals = jnp.expand_dims(goals, axis=0)\n",
    "            goals = jnp.expand_dims(goals, axis=1)\n",
    "\n",
    "        _, predicted_actions = self.network.select('target_q_predictors')(observations, goals, action_seq=None)\n",
    "        actions = discrete_to_continuous(predicted_actions, self.config['action_max'],\n",
    "                                         self.config['action_min'], self.config['num_bins'])  # Placeholder: directly use predicted discrete actions.\n",
    "\n",
    "        return actions[0, -1, :]\n",
    "\n",
    "    @classmethod\n",
    "    def create(\n",
    "        cls,\n",
    "        seed,\n",
    "        ex_observations,\n",
    "        ex_actions,\n",
    "        config,\n",
    "    ):\n",
    "        \"\"\"Create a new QMixer agent.\n",
    "        \n",
    "        Args:\n",
    "            seed: Random seed.\n",
    "            ex_observations: Example observation batch.\n",
    "            ex_actions: Example action batch (for discrete actions, expect max action value).\n",
    "            config: Configuration dictionary.\n",
    "        \"\"\"\n",
    "        print(\"Creating QMixer agent\")\n",
    "        rng = jax.random.PRNGKey(seed)\n",
    "        rng, init_rng = jax.random.split(rng, 2)\n",
    "\n",
    "        if len(ex_observations.shape) == 2:\n",
    "            ex_observations = jnp.expand_dims(ex_observations, axis=1)\n",
    "            ex_actions = jnp.expand_dims(ex_actions, axis=1)\n",
    "\n",
    "        ex_goals = ex_observations\n",
    "        if config['discrete']:\n",
    "            action_dim = jnp.max(ex_actions) + 1\n",
    "        else:\n",
    "            action_dim = ex_actions.shape[-1]\n",
    "\n",
    "        # Define encoder.\n",
    "        encoders = dict()\n",
    "        if config['encoder'] is not None:\n",
    "            encoder_module = encoder_modules[config['encoder']]\n",
    "            encoders['actor'] = GCEncoder(state_encoder=encoder_module(), goal_encoder=encoder_module())\n",
    "        \n",
    "        if config['discrete']:\n",
    "            q_predictors = None\n",
    "            raise NotImplementedError(\"Discrete actions not supported yet.\")\n",
    "        \n",
    "        else:\n",
    "            q_predictors = QFunctionMixerCore(\n",
    "                num_tokens=ex_observations.shape[1],\n",
    "                state_dim=config['feature_dim'],\n",
    "                num_action_dims=action_dim,\n",
    "                num_bins=config['num_bins'],\n",
    "                joint_embed_dim=config['feature_dim'],\n",
    "                num_mixer_blocks=config['num_mixer_blocks'],\n",
    "                mixer_token_hidden=config['mixer_hidden'],\n",
    "                mixer_channel_hidden=config['mixer_hidden'],\n",
    "                gc_encoder=encoders.get('actor')\n",
    "            )\n",
    "\n",
    "        ex_discrete_actions = continuous_to_discrete(ex_actions, config['action_max'], config['action_min'], config['num_bins'])\n",
    "\n",
    "        network_info = dict(\n",
    "            q_predictors=(q_predictors, (ex_observations, ex_goals, ex_discrete_actions)),\n",
    "            target_q_predictors=(copy.deepcopy(q_predictors), (ex_observations, ex_goals, ex_discrete_actions)),\n",
    "        )\n",
    "        networks = {k: v[0] for k, v in network_info.items()}\n",
    "        network_args = {k: v[1] for k, v in network_info.items()}\n",
    "\n",
    "        network_def = ModuleDict(networks)\n",
    "        network_tx = optax.adam(learning_rate=config['lr'])\n",
    "        network_params = network_def.init(init_rng, **network_args)['params']\n",
    "        network = TrainState.create(network_def, network_params, tx=network_tx)\n",
    "\n",
    "        params = network_params\n",
    "        params['modules_target_q_predictors'] = params['modules_q_predictors']\n",
    "\n",
    "        print(\"Creating Done\")\n",
    "\n",
    "        return cls(rng, network=network, config=flax.core.FrozenDict(**config))\n",
    "\n",
    "\n",
    "def discretize(actions):\n",
    "    actions = jnp.array(actions)\n",
    "    action_max = jnp.max(actions, axis=0)\n",
    "    action_min = jnp.min(actions, axis=0)\n",
    "\n",
    "    return action_max, action_min\n",
    "\n",
    "def discrete_to_continuous(action_logits, action_max, action_min, num_bins):\n",
    "    # Converts tensor of shape (B, V, A, N) to (B, V, A)\n",
    "    discrete_actions = action_logits.astype(jnp.float32)\n",
    "    continuous_actions = (discrete_actions / (num_bins - 1)) * (\n",
    "        action_max - action_min\n",
    "    ) + action_min\n",
    "    return continuous_actions\n",
    "\n",
    "def continuous_to_discrete(actions, action_max, action_min, num_bins):\n",
    "    actions = (actions - action_min) / (action_max - action_min)\n",
    "    actions = actions * (num_bins - 1)\n",
    "    actions = jnp.round(actions).astype(jnp.int32)  # shape (B, V, A)\n",
    "    actions = jnp.clip(actions, 0, num_bins - 1)\n",
    "    # actions = jax.nn.one_hot(actions, num_classes=num_bins)  # shape (B, V, A, num_bins)\n",
    "    return actions\n",
    "\n",
    "def get_config():\n",
    "    config = ml_collections.ConfigDict(\n",
    "        dict(\n",
    "            agent_name='qmixer',  # Agent name.\n",
    "            lr=3e-4,              # Learning rate.\n",
    "            batch_size=1024,      # Batch size.\n",
    "            discount=0.99,\n",
    "            alpha=1.0,\n",
    "            tau=0.005,  # Target network update rate.\n",
    "            const_std=True,\n",
    "            discrete=False,  # Whether the action space is discrete.\n",
    "            encoder=ml_collections.config_dict.placeholder(str),  # Visual encoder name (None, 'impala_small', etc.).\n",
    "            dataset_class='GCDataset',\n",
    "            value_p_curgoal=0.2,  # Probability of using the current state as the value goal.\n",
    "            value_p_trajgoal=0.8,  # Probability of using a future state in the same trajectory as the value goal.\n",
    "            value_p_randomgoal=0.0,  # Probability of using a random state as the value goal.\n",
    "            value_geom_sample=True,  # Whether to use geometric sampling for future value goals.\n",
    "            actor_p_curgoal=0.0,  # Probability of using the current state as the actor goal.\n",
    "            actor_p_trajgoal=1.0,  # Probability of using a future state in the same trajectory as the actor goal.\n",
    "            actor_p_randomgoal=0.0,  # Probability of using a random state as the actor goal.\n",
    "            actor_geom_sample=False,  # Whether to use geometric sampling for future actor goals.\n",
    "            gc_negative=True,  # Whether to use '0 if s == g else -1' (True) or '1 if s == g else 0' (False) as reward.\n",
    "            p_aug=0.0,  # Probability of applying image augmentation.\n",
    "            frame_stack=ml_collections.config_dict.placeholder(int),\n",
    "            feature_dim=10,\n",
    "            action_max=1.0,\n",
    "            action_min=-1.0,\n",
    "            num_bins=256,\n",
    "            # joint_embed_dim=64,\n",
    "            mixer_hidden=256,\n",
    "            num_mixer_blocks=1,\n",
    "        )\n",
    "    )\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating QMixer agent\n",
      "Update info: {'conservative_loss': Array(0.14575729, dtype=float32), 'grad/max': Array(0.22816157, dtype=float32), 'grad/min': Array(-0.7297418, dtype=float32), 'grad/norm': Array(6.0283675, dtype=float32), 'td_error': Array(0.4406067, dtype=float32), 'total_loss': Array(0.58636403, dtype=float32)}\n",
      "Sampled actions: [-0.9607843  -0.02745092]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import jax\n",
    "    import jax.numpy as jnp\n",
    "    import ml_collections\n",
    "\n",
    "    # Create a simple configuration with required placeholders filled.\n",
    "    config = get_config()\n",
    "    config.frame_stack = 4\n",
    "    config.encoder = \"mlp_small\"  # Example encoder name; adjust as needed.\n",
    "    # Optionally set num_bins if not defined in config.\n",
    "    # if 'num_bins' not in config:\n",
    "    #     config.num_bins = 256\n",
    "\n",
    "    # Dummy example parameters.\n",
    "    batch_size = 2\n",
    "    sequence_length = 5\n",
    "    feature_dim = 2\n",
    "    action_dim = 2  # For discrete actions, ex_actions shape or maximum value is used.\n",
    "\n",
    "    # Dummy observations (batch, sequence, features) and actions (batch, sequence, action_dim)\n",
    "    ex_observations = jnp.ones((batch_size, feature_dim))\n",
    "    ex_goals = jnp.ones((batch_size, feature_dim))\n",
    "    ex_rewards = jnp.ones((batch_size))\n",
    "    ex_actions = jnp.zeros((batch_size, action_dim), dtype=jnp.int32)\n",
    "\n",
    "    # Create the QMixer agent using the class method\n",
    "    agent = QMixerAgent.create(seed=0, ex_observations=ex_observations, ex_actions=ex_actions, config=config)\n",
    "\n",
    "    # Create a dummy batch that the agent expects.\n",
    "    # Note: The expected shape of 'td_targets' should match the output shape of the QMixer network.\n",
    "    batch = {\n",
    "        \"observations\": ex_observations,\n",
    "        \"next_observations\": ex_observations,\n",
    "        \"actions\": ex_actions,\n",
    "        \"rewards\": ex_rewards,\n",
    "        \"value_goals\": ex_goals,\n",
    "        \"mc_returns\": ex_rewards,\n",
    "    }\n",
    "\n",
    "    # Test the update function.\n",
    "    agent, info = agent.update(batch)\n",
    "    print(\"Update info:\", info)\n",
    "\n",
    "    # Test the sample_actions function.\n",
    "    ex_observations = jnp.ones((feature_dim))\n",
    "    ex_goals = jnp.ones((feature_dim))\n",
    "    sampled_actions = agent.sample_actions(ex_observations, ex_goals)\n",
    "    print(\"Sampled actions:\", sampled_actions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ogbench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
